## Организация кода

0. Распаковать dataset в корне ```data/```
1. Прогнать файл ```prepare-tokenizer.ipynb``` чтобы подготовить tokenizer (словарь)
2. ```main.py```
3. ```classify-test.py```

## Дневник эксперимента

В качестве токенизатора выбрал (относительно) новую библиотеку ```tokenizers``` от 
```huggingface```, чтобы попробовать что-то новое. В отличие от ```yttm```, в
ней можно задавать собственные специальные токены.

Для нормализации решил применить лишь приведение к lower case, попробовать 
без пунктуации не успел.

Обнаружилось, что в датасете не сбалансированы классы. Для решения я написал свой 
```dataset```, который выдаёт ровное количество True и False элементов на каждой 
эпохе. Он выровнен по True лейблу. Каждую эпоху выдаются все True элементы и 
на каждый элемент True сэмплится элемент из False класса (более детально в 
```src/dataset.py``` файле).

1 нейрон на выходе + threshold показывает себя хуже, чем 2 нейрона 
(по 1 для каждого класса)

Начал с старых примеров решения NLP задач:
1) BERT embedding -> classifier
2) Eng 2 Rus Attention
3) Embedding + LSTM

Наиболее точные результаты показывала последняя модель, поэтому продолжил 
твикать параметры именно для неё.

Во время проверки данных от токенизатора выяснилось, что библиотечные классы 
```tokenizers``` игнорируют указанные параметры и в итоге предобработка текста 
выполняется неверно. Т.е. всё это время токенизатор производил больший
размер словаря и не приводил текст к lower case. 
Максимум точности, что я смог добиться при данном токенизаторе *89,07%*. 

Перешёл на ```yttm``` библиотеку. С прежними гиперпараметрами системы точность 
выросла до *92,5%*
